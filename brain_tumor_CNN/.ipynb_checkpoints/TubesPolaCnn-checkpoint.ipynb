{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import imutils.paths as path\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from skimage.io import imread\n",
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Activation, Dense, Conv2D, MaxPooling2D, ZeroPadding2D, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "positifList=os.listdir('brain_tumor_dataset/yes/')\n",
    "negatifList=os.listdir('brain_tumor_dataset/no/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadCitra (listData, labeling):\n",
    "    data = []\n",
    "    label = []\n",
    "    for i in tqdm (listData):\n",
    "        if (labeling == 0):\n",
    "            sample = cv2.imread ('brain_tumor_dataset/yes/'+i)\n",
    "        elif (labeling == 1):\n",
    "            sample = cv2.imread ('brain_tumor_dataset/no/'+i)\n",
    "        sampleResize= cv2.resize (sample, (32,32) )\n",
    "        (b, g, r)=cv2.split(sampleResize) \n",
    "        sample2=cv2.merge([r,g,b])\n",
    "        data.append(sample2)\n",
    "        label.append(labeling)\n",
    "    return (data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:01<00:00, 136.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "(155, 32, 32, 3)\n",
      "panjang IDpositif:155\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:00<00:00, 167.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "(155, 32, 32, 3)\n",
      "panjang IDnegatif: 98\n",
      "\n",
      "(253, 32, 32, 3)\n",
      "Jumlah Citra: 253\n",
      "panjang ID: 253\n"
     ]
    }
   ],
   "source": [
    "citraPositif, IDpositif=loadCitra(positifList, 0)\n",
    "citraArray = np.array (citraPositif)\n",
    "print (IDpositif)\n",
    "print (citraArray.shape)\n",
    "print(\"panjang IDpositif:\" + str(len(IDpositif))+\"\\n\")\n",
    "\n",
    "citraNegatif, IDnegatif=loadCitra(negatifList, 1)\n",
    "citraArray = np.array (citraPositif)\n",
    "print (IDnegatif)\n",
    "print (citraArray.shape)\n",
    "print(\"panjang IDnegatif: \" + str(len(IDnegatif))+\"\\n\")\n",
    "\n",
    "citra = citraPositif+citraNegatif\n",
    "ex=np.array (citra)\n",
    "print (ex.shape)\n",
    "ID=IDpositif+IDnegatif\n",
    "print(\"Jumlah Citra: \" + str(len(citra)))\n",
    "print(\"panjang ID: \" + str(len(ID)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "ID=np.array(ID)\n",
    "\n",
    "print (ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalisasi data\n",
    "citra = np.squeeze(citra)\n",
    "citra = citra.astype('float32')\n",
    "citra /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d545905d68>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(citra[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, l_train, l_test =  train_test_split(citra, ID, stratify=ID, test_size=0.3, random_state=5)\n",
    "y_train=to_categorical(l_train) \n",
    "y_test= to_categorical(l_test)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(l_test.shape)\n",
    "print(y_test.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Feature Extraction Layer \n",
    "inputs = Input(shape=(32, 32, 3))\n",
    "conv_layer = ZeroPadding2D(padding=(2,2))(inputs)\n",
    "conv_layer = Conv2D(32, (3, 3), strides=(1,1), activation='relu')(conv_layer)\n",
    "conv_layer = MaxPooling2D((2, 2))(conv_layer)\n",
    "conv_layer = Conv2D(64, (3, 3), strides=(1,1), activation='relu')(conv_layer)\n",
    "conv_layer = MaxPooling2D((2, 2))(conv_layer)\n",
    "conv_layer = Conv2D(128, (3, 3), strides=(1,1), activation='relu')(conv_layer)\n",
    "\n",
    "# 4.2 Flatten feature map to Vector with 576 element.\n",
    "flatten = Flatten()(conv_layer)\n",
    "\n",
    "# 4.3 Fully Connected Layer (classification layer)\n",
    "fc_layer = Dense(128, activation='relu')(flatten)\n",
    "fc_layer = Dense(64, activation='relu')(fc_layer)\n",
    "outputs = Dense(2, activation='softmax')(fc_layer)\n",
    "\n",
    "modelFaceRec = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Gunakan Adam Optimizer and categorical_crossentropy loss evaluation\n",
    "\n",
    "modelFaceRec.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#model1.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 4.5 Print Model Summary\n",
    "print(modelFaceRec.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 100 Epochs and use TensorBoard Callback\n",
    "history=modelFaceRec.fit(x_train, y_train, epochs=250, verbose=0, validation_data=(x_test, y_test))\n",
    "\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "test_loss, test_acc = modelFaceRec.evaluate(x_train, y_train)\n",
    "\n",
    "print('Trainning accuracy:', test_acc)\n",
    "\n",
    "test_loss, test_acc = modelFaceRec.evaluate(x_test, y_test)\n",
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = (citra[:190],ID[:190]) , (citra[190:] , ID[190:])\n",
    "(x_valid , y_valid) = (x_test[:63], y_test[:63])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 16)        3904      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 16)        20752     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 36)          46692     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 36)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 36)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               295424    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 367,285\n",
      "Trainable params: 367,285\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Must define the input shape in the first layer of the neural network\n",
    "model.add(tf.keras.layers.Conv2D(filters=16,kernel_size=9, padding='same', activation='relu', input_shape=(32,32,3))) \n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model.add(tf.keras.layers.Dropout(0.45))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=16,kernel_size=9,padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=36, kernel_size=9, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.15))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Take a look at the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer=tf.keras.optimizers.Adam(),\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7615 - acc: 0.203 - ETA: 0s - loss: 0.6624 - acc: 0.405 - 1s 448ms/step - loss: 0.6624 - acc: 0.4053 - val_loss: 2.0167 - val_acc: 0.0000e+00\n",
      "Epoch 2/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7782 - acc: 0.781 - ETA: 0s - loss: 0.6327 - acc: 0.815 - 0s 180ms/step - loss: 0.6327 - acc: 0.8158 - val_loss: 1.0038 - val_acc: 0.0000e+00\n",
      "Epoch 3/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4548 - acc: 0.828 - ETA: 0s - loss: 0.4713 - acc: 0.815 - 0s 172ms/step - loss: 0.4713 - acc: 0.8158 - val_loss: 0.9105 - val_acc: 0.0000e+00\n",
      "Epoch 4/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4788 - acc: 0.835 - ETA: 0s - loss: 0.4955 - acc: 0.815 - 0s 190ms/step - loss: 0.4955 - acc: 0.8158 - val_loss: 1.0082 - val_acc: 0.0000e+00\n",
      "Epoch 5/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4636 - acc: 0.820 - ETA: 0s - loss: 0.4642 - acc: 0.815 - 0s 196ms/step - loss: 0.4642 - acc: 0.8158 - val_loss: 1.2533 - val_acc: 0.0000e+00\n",
      "Epoch 6/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4789 - acc: 0.804 - ETA: 0s - loss: 0.4608 - acc: 0.815 - 0s 213ms/step - loss: 0.4608 - acc: 0.8158 - val_loss: 1.3719 - val_acc: 0.0000e+00\n",
      "Epoch 7/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4441 - acc: 0.820 - ETA: 0s - loss: 0.4698 - acc: 0.815 - 0s 202ms/step - loss: 0.4698 - acc: 0.8158 - val_loss: 1.1779 - val_acc: 0.0000e+00\n",
      "Epoch 8/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.5154 - acc: 0.773 - ETA: 0s - loss: 0.4642 - acc: 0.815 - 0s 185ms/step - loss: 0.4642 - acc: 0.8158 - val_loss: 1.0077 - val_acc: 0.0000e+00\n",
      "Epoch 9/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4572 - acc: 0.820 - ETA: 0s - loss: 0.4629 - acc: 0.815 - 0s 190ms/step - loss: 0.4629 - acc: 0.8158 - val_loss: 1.0318 - val_acc: 0.0000e+00\n",
      "Epoch 10/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4241 - acc: 0.835 - ETA: 0s - loss: 0.4574 - acc: 0.815 - 0s 200ms/step - loss: 0.4574 - acc: 0.8158 - val_loss: 1.1315 - val_acc: 0.0000e+00\n",
      "Epoch 11/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4441 - acc: 0.820 - ETA: 0s - loss: 0.4518 - acc: 0.815 - 0s 191ms/step - loss: 0.4518 - acc: 0.8158 - val_loss: 1.1843 - val_acc: 0.0000e+00\n",
      "Epoch 12/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4589 - acc: 0.812 - ETA: 0s - loss: 0.4474 - acc: 0.815 - 0s 192ms/step - loss: 0.4474 - acc: 0.8158 - val_loss: 1.1593 - val_acc: 0.0000e+00\n",
      "Epoch 13/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4251 - acc: 0.828 - ETA: 0s - loss: 0.4479 - acc: 0.815 - 0s 177ms/step - loss: 0.4479 - acc: 0.8158 - val_loss: 1.0759 - val_acc: 0.0000e+00\n",
      "Epoch 14/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4137 - acc: 0.835 - ETA: 0s - loss: 0.4383 - acc: 0.815 - 0s 168ms/step - loss: 0.4383 - acc: 0.8158 - val_loss: 1.0108 - val_acc: 0.0000e+00\n",
      "Epoch 15/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4478 - acc: 0.812 - ETA: 0s - loss: 0.4394 - acc: 0.815 - 0s 180ms/step - loss: 0.4394 - acc: 0.8158 - val_loss: 1.0478 - val_acc: 0.0000e+00\n",
      "Epoch 16/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.3566 - acc: 0.875 - ETA: 0s - loss: 0.4546 - acc: 0.815 - 0s 179ms/step - loss: 0.4546 - acc: 0.8158 - val_loss: 1.0622 - val_acc: 0.0000e+00\n",
      "Epoch 17/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4893 - acc: 0.796 - ETA: 0s - loss: 0.4494 - acc: 0.815 - 0s 190ms/step - loss: 0.4494 - acc: 0.8158 - val_loss: 0.9456 - val_acc: 0.0000e+00\n",
      "Epoch 18/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4315 - acc: 0.804 - ETA: 0s - loss: 0.4339 - acc: 0.815 - 0s 191ms/step - loss: 0.4339 - acc: 0.8158 - val_loss: 1.0466 - val_acc: 0.0000e+00\n",
      "Epoch 19/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4312 - acc: 0.828 - ETA: 0s - loss: 0.4404 - acc: 0.815 - 0s 181ms/step - loss: 0.4404 - acc: 0.8158 - val_loss: 1.1354 - val_acc: 0.0000e+00\n",
      "Epoch 20/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4646 - acc: 0.789 - ETA: 0s - loss: 0.4355 - acc: 0.815 - 0s 180ms/step - loss: 0.4355 - acc: 0.8158 - val_loss: 1.0648 - val_acc: 0.0000e+00\n",
      "Epoch 21/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4478 - acc: 0.804 - ETA: 0s - loss: 0.4254 - acc: 0.815 - 0s 185ms/step - loss: 0.4254 - acc: 0.8158 - val_loss: 0.9961 - val_acc: 0.0000e+00\n",
      "Epoch 22/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4035 - acc: 0.828 - ETA: 0s - loss: 0.4189 - acc: 0.815 - 0s 184ms/step - loss: 0.4189 - acc: 0.8158 - val_loss: 0.9842 - val_acc: 0.0000e+00\n",
      "Epoch 23/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4394 - acc: 0.812 - ETA: 0s - loss: 0.4311 - acc: 0.815 - 0s 190ms/step - loss: 0.4311 - acc: 0.8158 - val_loss: 0.9737 - val_acc: 0.0000e+00\n",
      "Epoch 24/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.3940 - acc: 0.828 - ETA: 0s - loss: 0.4204 - acc: 0.815 - 0s 187ms/step - loss: 0.4204 - acc: 0.8158 - val_loss: 0.9873 - val_acc: 0.0000e+00\n",
      "Epoch 25/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.3243 - acc: 0.867 - ETA: 0s - loss: 0.4235 - acc: 0.815 - 0s 202ms/step - loss: 0.4235 - acc: 0.8158 - val_loss: 0.8640 - val_acc: 0.0000e+00\n",
      "Epoch 26/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4095 - acc: 0.820 - ETA: 0s - loss: 0.4107 - acc: 0.815 - 0s 191ms/step - loss: 0.4107 - acc: 0.8158 - val_loss: 0.8497 - val_acc: 0.0000e+00\n",
      "Epoch 27/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4337 - acc: 0.820 - ETA: 0s - loss: 0.4211 - acc: 0.815 - 0s 177ms/step - loss: 0.4211 - acc: 0.8158 - val_loss: 1.1082 - val_acc: 0.0000e+00\n",
      "Epoch 28/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.3574 - acc: 0.859 - ETA: 0s - loss: 0.4555 - acc: 0.815 - 1s 270ms/step - loss: 0.4555 - acc: 0.8158 - val_loss: 1.0844 - val_acc: 0.0000e+00\n",
      "Epoch 29/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.3972 - acc: 0.828 - ETA: 0s - loss: 0.3940 - acc: 0.815 - 0s 168ms/step - loss: 0.3940 - acc: 0.8158 - val_loss: 0.8115 - val_acc: 0.2063\n",
      "Epoch 30/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4279 - acc: 0.828 - ETA: 0s - loss: 0.4428 - acc: 0.826 - 0s 184ms/step - loss: 0.4428 - acc: 0.8263 - val_loss: 0.8631 - val_acc: 0.1746\n",
      "Epoch 31/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4218 - acc: 0.804 - ETA: 0s - loss: 0.4022 - acc: 0.815 - 0s 173ms/step - loss: 0.4022 - acc: 0.8158 - val_loss: 1.1710 - val_acc: 0.0794\n",
      "Epoch 32/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.3742 - acc: 0.851 - ETA: 0s - loss: 0.4339 - acc: 0.815 - 0s 181ms/step - loss: 0.4339 - acc: 0.8158 - val_loss: 1.2391 - val_acc: 0.0794\n",
      "Epoch 33/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4006 - acc: 0.820 - ETA: 0s - loss: 0.4181 - acc: 0.815 - 0s 181ms/step - loss: 0.4181 - acc: 0.8158 - val_loss: 0.9545 - val_acc: 0.1429\n",
      "Epoch 34/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.3395 - acc: 0.875 - ETA: 0s - loss: 0.3969 - acc: 0.826 - 0s 183ms/step - loss: 0.3969 - acc: 0.8263 - val_loss: 0.8522 - val_acc: 0.2381\n",
      "Epoch 35/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4345 - acc: 0.843 - ETA: 0s - loss: 0.4302 - acc: 0.842 - 0s 186ms/step - loss: 0.4302 - acc: 0.8421 - val_loss: 0.9243 - val_acc: 0.1746\n",
      "Epoch 36/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4042 - acc: 0.843 - ETA: 0s - loss: 0.3956 - acc: 0.836 - 0s 175ms/step - loss: 0.3956 - acc: 0.8368 - val_loss: 1.1525 - val_acc: 0.1111\n",
      "Epoch 37/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4079 - acc: 0.835 - ETA: 0s - loss: 0.3963 - acc: 0.826 - 0s 168ms/step - loss: 0.3963 - acc: 0.8263 - val_loss: 1.2690 - val_acc: 0.1111\n",
      "Epoch 38/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4232 - acc: 0.828 - ETA: 0s - loss: 0.4109 - acc: 0.831 - 0s 181ms/step - loss: 0.4109 - acc: 0.8316 - val_loss: 1.0992 - val_acc: 0.1587\n",
      "Epoch 39/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4186 - acc: 0.812 - ETA: 0s - loss: 0.3944 - acc: 0.842 - 0s 180ms/step - loss: 0.3944 - acc: 0.8421 - val_loss: 0.9366 - val_acc: 0.3333\n",
      "Epoch 40/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.3587 - acc: 0.898 - ETA: 0s - loss: 0.3895 - acc: 0.873 - 0s 185ms/step - loss: 0.3895 - acc: 0.8737 - val_loss: 0.9613 - val_acc: 0.3492\n",
      "Epoch 41/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.3932 - acc: 0.867 - ETA: 0s - loss: 0.3721 - acc: 0.857 - 0s 176ms/step - loss: 0.3721 - acc: 0.8579 - val_loss: 1.0798 - val_acc: 0.2540\n",
      "Epoch 42/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.3514 - acc: 0.843 - ETA: 0s - loss: 0.3778 - acc: 0.831 - 0s 182ms/step - loss: 0.3778 - acc: 0.8316 - val_loss: 1.1416 - val_acc: 0.2698\n",
      "Epoch 43/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4210 - acc: 0.828 - ETA: 0s - loss: 0.3796 - acc: 0.852 - 0s 169ms/step - loss: 0.3796 - acc: 0.8526 - val_loss: 1.0031 - val_acc: 0.4127\n",
      "Epoch 44/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.3611 - acc: 0.875 - ETA: 0s - loss: 0.3677 - acc: 0.863 - 0s 185ms/step - loss: 0.3677 - acc: 0.8632 - val_loss: 1.0566 - val_acc: 0.3810\n",
      "Epoch 45/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.3529 - acc: 0.867 - ETA: 0s - loss: 0.3711 - acc: 0.863 - 0s 196ms/step - loss: 0.3711 - acc: 0.8632 - val_loss: 1.0838 - val_acc: 0.3651\n",
      "Epoch 46/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.3172 - acc: 0.906 - ETA: 0s - loss: 0.3567 - acc: 0.878 - 0s 211ms/step - loss: 0.3567 - acc: 0.8789 - val_loss: 0.9951 - val_acc: 0.4286\n",
      "Epoch 47/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.3724 - acc: 0.867 - ETA: 0s - loss: 0.3648 - acc: 0.873 - 0s 223ms/step - loss: 0.3648 - acc: 0.8737 - val_loss: 1.0833 - val_acc: 0.3175\n",
      "Epoch 48/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.3512 - acc: 0.875 - ETA: 0s - loss: 0.3576 - acc: 0.863 - 0s 205ms/step - loss: 0.3576 - acc: 0.8632 - val_loss: 1.3006 - val_acc: 0.1746\n",
      "Epoch 49/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.3706 - acc: 0.851 - ETA: 0s - loss: 0.3665 - acc: 0.852 - 0s 184ms/step - loss: 0.3665 - acc: 0.8526 - val_loss: 1.1183 - val_acc: 0.2857\n",
      "Epoch 50/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.3614 - acc: 0.867 - ETA: 0s - loss: 0.3487 - acc: 0.884 - 0s 199ms/step - loss: 0.3487 - acc: 0.8842 - val_loss: 0.9574 - val_acc: 0.4921\n",
      "Epoch 51/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.3712 - acc: 0.835 - ETA: 0s - loss: 0.3503 - acc: 0.852 - 0s 186ms/step - loss: 0.3503 - acc: 0.8526 - val_loss: 1.2605 - val_acc: 0.2857\n",
      "Epoch 52/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.3108 - acc: 0.890 - ETA: 0s - loss: 0.3527 - acc: 0.863 - 0s 189ms/step - loss: 0.3527 - acc: 0.8632 - val_loss: 1.4603 - val_acc: 0.2063\n",
      "Epoch 53/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.3197 - acc: 0.890 - ETA: 0s - loss: 0.3309 - acc: 0.884 - 0s 182ms/step - loss: 0.3309 - acc: 0.8842 - val_loss: 0.9523 - val_acc: 0.5238\n",
      "Epoch 54/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.3117 - acc: 0.890 - ETA: 0s - loss: 0.3271 - acc: 0.873 - 0s 173ms/step - loss: 0.3271 - acc: 0.8737 - val_loss: 1.0567 - val_acc: 0.4921\n",
      "Epoch 55/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2852 - acc: 0.882 - ETA: 0s - loss: 0.3196 - acc: 0.857 - 0s 181ms/step - loss: 0.3196 - acc: 0.8579 - val_loss: 1.4784 - val_acc: 0.1746\n",
      "Epoch 56/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.3667 - acc: 0.859 - ETA: 0s - loss: 0.3522 - acc: 0.863 - 0s 205ms/step - loss: 0.3522 - acc: 0.8632 - val_loss: 1.3025 - val_acc: 0.2222\n",
      "Epoch 57/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.3731 - acc: 0.843 - ETA: 0s - loss: 0.3336 - acc: 0.863 - 0s 179ms/step - loss: 0.3336 - acc: 0.8632 - val_loss: 1.1082 - val_acc: 0.3333\n",
      "Epoch 58/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.3048 - acc: 0.898 - ETA: 0s - loss: 0.3232 - acc: 0.868 - 0s 181ms/step - loss: 0.3232 - acc: 0.8684 - val_loss: 1.2328 - val_acc: 0.3175\n",
      "Epoch 59/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.3276 - acc: 0.890 - ETA: 0s - loss: 0.3002 - acc: 0.884 - 0s 189ms/step - loss: 0.3002 - acc: 0.8842 - val_loss: 1.3999 - val_acc: 0.3175\n",
      "Epoch 60/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2562 - acc: 0.898 - ETA: 0s - loss: 0.2966 - acc: 0.884 - 0s 192ms/step - loss: 0.2966 - acc: 0.8842 - val_loss: 1.2605 - val_acc: 0.4444\n",
      "Epoch 61/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2568 - acc: 0.875 - ETA: 0s - loss: 0.2873 - acc: 0.873 - 0s 187ms/step - loss: 0.2873 - acc: 0.8737 - val_loss: 1.0787 - val_acc: 0.5873\n",
      "Epoch 62/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2846 - acc: 0.890 - ETA: 0s - loss: 0.2922 - acc: 0.905 - 0s 186ms/step - loss: 0.2922 - acc: 0.9053 - val_loss: 1.5785 - val_acc: 0.2857\n",
      "Epoch 63/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.3130 - acc: 0.851 - ETA: 0s - loss: 0.3038 - acc: 0.863 - 0s 168ms/step - loss: 0.3038 - acc: 0.8632 - val_loss: 1.4117 - val_acc: 0.3175\n",
      "Epoch 64/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2328 - acc: 0.906 - ETA: 0s - loss: 0.2742 - acc: 0.889 - 0s 180ms/step - loss: 0.2742 - acc: 0.8895 - val_loss: 1.1316 - val_acc: 0.4286\n",
      "Epoch 65/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2844 - acc: 0.890 - ETA: 0s - loss: 0.2890 - acc: 0.900 - 0s 181ms/step - loss: 0.2890 - acc: 0.9000 - val_loss: 1.3137 - val_acc: 0.3492\n",
      "Epoch 66/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2633 - acc: 0.898 - ETA: 0s - loss: 0.2628 - acc: 0.894 - 0s 173ms/step - loss: 0.2628 - acc: 0.8947 - val_loss: 1.4573 - val_acc: 0.3175\n",
      "Epoch 67/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2092 - acc: 0.929 - ETA: 0s - loss: 0.2500 - acc: 0.900 - 0s 197ms/step - loss: 0.2500 - acc: 0.9000 - val_loss: 1.0283 - val_acc: 0.5873\n",
      "Epoch 68/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.3057 - acc: 0.867 - ETA: 0s - loss: 0.2660 - acc: 0.900 - 0s 192ms/step - loss: 0.2660 - acc: 0.9000 - val_loss: 1.7579 - val_acc: 0.3175\n",
      "Epoch 69/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2728 - acc: 0.914 - ETA: 0s - loss: 0.2570 - acc: 0.921 - 0s 184ms/step - loss: 0.2570 - acc: 0.9211 - val_loss: 1.8501 - val_acc: 0.3492\n",
      "Epoch 70/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2201 - acc: 0.929 - ETA: 0s - loss: 0.2117 - acc: 0.921 - 0s 196ms/step - loss: 0.2117 - acc: 0.9211 - val_loss: 1.3879 - val_acc: 0.5873\n",
      "Epoch 71/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2281 - acc: 0.914 - ETA: 0s - loss: 0.2285 - acc: 0.921 - 0s 180ms/step - loss: 0.2285 - acc: 0.9211 - val_loss: 1.8445 - val_acc: 0.3492\n",
      "Epoch 72/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.1945 - acc: 0.914 - ETA: 0s - loss: 0.2184 - acc: 0.910 - 0s 165ms/step - loss: 0.2184 - acc: 0.9105 - val_loss: 1.3945 - val_acc: 0.5238\n",
      "Epoch 73/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.1989 - acc: 0.945 - ETA: 0s - loss: 0.2067 - acc: 0.942 - 0s 196ms/step - loss: 0.2067 - acc: 0.9421 - val_loss: 1.3901 - val_acc: 0.5238\n",
      "Epoch 74/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2378 - acc: 0.914 - ETA: 0s - loss: 0.2058 - acc: 0.926 - 0s 187ms/step - loss: 0.2058 - acc: 0.9263 - val_loss: 1.8577 - val_acc: 0.3333\n",
      "Epoch 75/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2066 - acc: 0.906 - ETA: 0s - loss: 0.1945 - acc: 0.915 - 0s 183ms/step - loss: 0.1945 - acc: 0.9158 - val_loss: 1.3479 - val_acc: 0.6508\n",
      "Epoch 76/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2586 - acc: 0.882 - ETA: 0s - loss: 0.2506 - acc: 0.894 - 0s 188ms/step - loss: 0.2506 - acc: 0.8947 - val_loss: 1.8081 - val_acc: 0.3492\n",
      "Epoch 77/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2044 - acc: 0.937 - ETA: 0s - loss: 0.2022 - acc: 0.926 - 0s 176ms/step - loss: 0.2022 - acc: 0.9263 - val_loss: 1.1673 - val_acc: 0.6349\n",
      "Epoch 78/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2399 - acc: 0.906 - ETA: 0s - loss: 0.2353 - acc: 0.900 - 0s 167ms/step - loss: 0.2353 - acc: 0.9000 - val_loss: 1.5645 - val_acc: 0.3968\n",
      "Epoch 79/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.1754 - acc: 0.937 - ETA: 0s - loss: 0.1971 - acc: 0.931 - 0s 181ms/step - loss: 0.1971 - acc: 0.9316 - val_loss: 1.7103 - val_acc: 0.3492\n",
      "Epoch 80/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.1496 - acc: 0.937 - ETA: 0s - loss: 0.1517 - acc: 0.936 - 0s 177ms/step - loss: 0.1517 - acc: 0.9368 - val_loss: 1.7997 - val_acc: 0.3810\n",
      "Epoch 81/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.1518 - acc: 0.945 - ETA: 0s - loss: 0.1667 - acc: 0.936 - 0s 179ms/step - loss: 0.1667 - acc: 0.9368 - val_loss: 1.8047 - val_acc: 0.3810\n",
      "Epoch 82/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.1309 - acc: 0.945 - ETA: 0s - loss: 0.1325 - acc: 0.947 - 0s 173ms/step - loss: 0.1325 - acc: 0.9474 - val_loss: 1.5181 - val_acc: 0.5873\n",
      "Epoch 83/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.1666 - acc: 0.945 - ETA: 0s - loss: 0.1994 - acc: 0.931 - 0s 185ms/step - loss: 0.1994 - acc: 0.9316 - val_loss: 1.9766 - val_acc: 0.2540\n",
      "Epoch 84/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2406 - acc: 0.890 - ETA: 0s - loss: 0.1913 - acc: 0.915 - 0s 178ms/step - loss: 0.1913 - acc: 0.9158 - val_loss: 1.4992 - val_acc: 0.5556\n",
      "Epoch 85/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.1464 - acc: 0.937 - ETA: 0s - loss: 0.1482 - acc: 0.931 - 0s 199ms/step - loss: 0.1482 - acc: 0.9316 - val_loss: 1.5430 - val_acc: 0.5079\n",
      "Epoch 86/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.1241 - acc: 0.968 - ETA: 0s - loss: 0.1231 - acc: 0.957 - 0s 203ms/step - loss: 0.1231 - acc: 0.9579 - val_loss: 2.2688 - val_acc: 0.3016\n",
      "Epoch 87/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.1532 - acc: 0.929 - ETA: 0s - loss: 0.1546 - acc: 0.942 - 0s 193ms/step - loss: 0.1546 - acc: 0.9421 - val_loss: 1.2356 - val_acc: 0.6984\n",
      "Epoch 88/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.1623 - acc: 0.929 - ETA: 0s - loss: 0.1660 - acc: 0.936 - 0s 192ms/step - loss: 0.1660 - acc: 0.9368 - val_loss: 1.6835 - val_acc: 0.6349\n",
      "Epoch 89/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.1279 - acc: 0.953 - ETA: 0s - loss: 0.1268 - acc: 0.947 - 0s 185ms/step - loss: 0.1268 - acc: 0.9474 - val_loss: 2.1086 - val_acc: 0.3810\n",
      "Epoch 90/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.1788 - acc: 0.953 - ETA: 0s - loss: 0.1398 - acc: 0.963 - 0s 206ms/step - loss: 0.1398 - acc: 0.9632 - val_loss: 1.6809 - val_acc: 0.6349\n",
      "Epoch 91/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0850 - acc: 0.968 - ETA: 0s - loss: 0.1116 - acc: 0.963 - 0s 189ms/step - loss: 0.1116 - acc: 0.9632 - val_loss: 1.6451 - val_acc: 0.6349\n",
      "Epoch 92/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.1343 - acc: 0.960 - ETA: 0s - loss: 0.1122 - acc: 0.968 - 0s 172ms/step - loss: 0.1122 - acc: 0.9684 - val_loss: 1.7339 - val_acc: 0.6032\n",
      "Epoch 93/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.1135 - acc: 0.968 - ETA: 0s - loss: 0.0905 - acc: 0.978 - 0s 164ms/step - loss: 0.0905 - acc: 0.9789 - val_loss: 1.6280 - val_acc: 0.6508\n",
      "Epoch 94/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0884 - acc: 0.976 - ETA: 0s - loss: 0.1094 - acc: 0.963 - 0s 172ms/step - loss: 0.1094 - acc: 0.9632 - val_loss: 1.8742 - val_acc: 0.6190\n",
      "Epoch 95/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0911 - acc: 0.968 - ETA: 0s - loss: 0.1225 - acc: 0.947 - 0s 175ms/step - loss: 0.1225 - acc: 0.9474 - val_loss: 1.9266 - val_acc: 0.5714\n",
      "Epoch 96/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.1382 - acc: 0.929 - ETA: 0s - loss: 0.1208 - acc: 0.942 - 0s 180ms/step - loss: 0.1208 - acc: 0.9421 - val_loss: 1.7510 - val_acc: 0.5238\n",
      "Epoch 97/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0743 - acc: 0.976 - ETA: 0s - loss: 0.0937 - acc: 0.963 - 0s 183ms/step - loss: 0.0937 - acc: 0.9632 - val_loss: 2.0933 - val_acc: 0.3333\n",
      "Epoch 98/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0881 - acc: 0.976 - ETA: 0s - loss: 0.0915 - acc: 0.968 - 0s 173ms/step - loss: 0.0915 - acc: 0.9684 - val_loss: 1.7322 - val_acc: 0.5556\n",
      "Epoch 99/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0854 - acc: 0.960 - ETA: 0s - loss: 0.0696 - acc: 0.973 - 0s 189ms/step - loss: 0.0696 - acc: 0.9737 - val_loss: 1.8551 - val_acc: 0.5873\n",
      "Epoch 100/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.1007 - acc: 0.945 - ETA: 0s - loss: 0.0873 - acc: 0.963 - 0s 181ms/step - loss: 0.0873 - acc: 0.9632 - val_loss: 2.1243 - val_acc: 0.5873\n",
      "Epoch 101/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0945 - acc: 0.968 - ETA: 0s - loss: 0.1050 - acc: 0.968 - 0s 187ms/step - loss: 0.1050 - acc: 0.9684 - val_loss: 2.2688 - val_acc: 0.5873\n",
      "Epoch 102/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0484 - acc: 0.984 - ETA: 0s - loss: 0.0764 - acc: 0.968 - 0s 179ms/step - loss: 0.0764 - acc: 0.9684 - val_loss: 1.6907 - val_acc: 0.7460\n",
      "Epoch 103/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.1078 - acc: 0.945 - ETA: 0s - loss: 0.0875 - acc: 0.963 - 0s 188ms/step - loss: 0.0875 - acc: 0.9632 - val_loss: 1.9951 - val_acc: 0.6190\n",
      "Epoch 104/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0745 - acc: 0.953 - ETA: 0s - loss: 0.0899 - acc: 0.952 - 0s 190ms/step - loss: 0.0899 - acc: 0.9526 - val_loss: 2.2424 - val_acc: 0.5079\n",
      "Epoch 105/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.1053 - acc: 0.953 - ETA: 0s - loss: 0.0845 - acc: 0.968 - 0s 189ms/step - loss: 0.0845 - acc: 0.9684 - val_loss: 1.2303 - val_acc: 0.7460\n",
      "Epoch 106/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0900 - acc: 0.976 - ETA: 0s - loss: 0.0882 - acc: 0.984 - 0s 219ms/step - loss: 0.0882 - acc: 0.9842 - val_loss: 1.9681 - val_acc: 0.5556\n",
      "Epoch 107/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0455 - acc: 0.984 - ETA: 0s - loss: 0.1049 - acc: 0.963 - 0s 216ms/step - loss: 0.1049 - acc: 0.9632 - val_loss: 2.2236 - val_acc: 0.5714\n",
      "Epoch 108/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0790 - acc: 0.960 - ETA: 0s - loss: 0.0707 - acc: 0.963 - 0s 207ms/step - loss: 0.0707 - acc: 0.9632 - val_loss: 1.3482 - val_acc: 0.7619\n",
      "Epoch 109/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0780 - acc: 0.968 - ETA: 0s - loss: 0.0831 - acc: 0.963 - 1s 301ms/step - loss: 0.0831 - acc: 0.9632 - val_loss: 2.0455 - val_acc: 0.6508\n",
      "Epoch 110/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0411 - acc: 0.984 - ETA: 0s - loss: 0.0770 - acc: 0.978 - 0s 205ms/step - loss: 0.0770 - acc: 0.9789 - val_loss: 2.3539 - val_acc: 0.5397\n",
      "Epoch 111/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0873 - acc: 0.937 - ETA: 0s - loss: 0.0823 - acc: 0.947 - 0s 186ms/step - loss: 0.0823 - acc: 0.9474 - val_loss: 1.5921 - val_acc: 0.6667\n",
      "Epoch 112/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.1343 - acc: 0.937 - ETA: 0s - loss: 0.1075 - acc: 0.957 - 0s 188ms/step - loss: 0.1075 - acc: 0.9579 - val_loss: 2.2674 - val_acc: 0.4762\n",
      "Epoch 113/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0932 - acc: 0.960 - ETA: 0s - loss: 0.0884 - acc: 0.957 - 0s 199ms/step - loss: 0.0884 - acc: 0.9579 - val_loss: 1.9122 - val_acc: 0.5238\n",
      "Epoch 114/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0592 - acc: 0.976 - ETA: 0s - loss: 0.0546 - acc: 0.984 - 0s 191ms/step - loss: 0.0546 - acc: 0.9842 - val_loss: 1.4442 - val_acc: 0.7143\n",
      "Epoch 115/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.1176 - acc: 0.945 - ETA: 0s - loss: 0.1007 - acc: 0.957 - 0s 201ms/step - loss: 0.1007 - acc: 0.9579 - val_loss: 1.6654 - val_acc: 0.7143\n",
      "Epoch 116/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0893 - acc: 0.960 - ETA: 0s - loss: 0.0765 - acc: 0.973 - 0s 198ms/step - loss: 0.0765 - acc: 0.9737 - val_loss: 2.2561 - val_acc: 0.6349\n",
      "Epoch 117/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0537 - acc: 0.992 - ETA: 0s - loss: 0.0741 - acc: 0.989 - 0s 186ms/step - loss: 0.0741 - acc: 0.9895 - val_loss: 2.3895 - val_acc: 0.6825\n",
      "Epoch 118/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0673 - acc: 0.976 - ETA: 0s - loss: 0.0773 - acc: 0.968 - 0s 180ms/step - loss: 0.0773 - acc: 0.9684 - val_loss: 2.0657 - val_acc: 0.7143\n",
      "Epoch 119/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0614 - acc: 0.968 - ETA: 0s - loss: 0.0613 - acc: 0.968 - 0s 194ms/step - loss: 0.0613 - acc: 0.9684 - val_loss: 2.2747 - val_acc: 0.6667\n",
      "Epoch 120/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0367 - acc: 0.984 - ETA: 0s - loss: 0.0350 - acc: 0.989 - 0s 194ms/step - loss: 0.0350 - acc: 0.9895 - val_loss: 2.4050 - val_acc: 0.6032\n",
      "Epoch 121/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0383 - acc: 1.000 - ETA: 0s - loss: 0.0296 - acc: 1.000 - 0s 193ms/step - loss: 0.0296 - acc: 1.0000 - val_loss: 2.3416 - val_acc: 0.6667\n",
      "Epoch 122/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0290 - acc: 0.984 - ETA: 0s - loss: 0.0377 - acc: 0.984 - 0s 189ms/step - loss: 0.0377 - acc: 0.9842 - val_loss: 2.3571 - val_acc: 0.6667\n",
      "Epoch 123/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0735 - acc: 0.984 - ETA: 0s - loss: 0.0528 - acc: 0.989 - 0s 201ms/step - loss: 0.0528 - acc: 0.9895 - val_loss: 2.0774 - val_acc: 0.6825\n",
      "Epoch 124/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0346 - acc: 0.992 - ETA: 0s - loss: 0.0323 - acc: 0.994 - 0s 209ms/step - loss: 0.0323 - acc: 0.9947 - val_loss: 2.0669 - val_acc: 0.6984\n",
      "Epoch 125/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0467 - acc: 0.984 - ETA: 0s - loss: 0.0433 - acc: 0.984 - 0s 194ms/step - loss: 0.0433 - acc: 0.9842 - val_loss: 2.5127 - val_acc: 0.5556\n",
      "Epoch 126/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0237 - acc: 0.992 - ETA: 0s - loss: 0.0384 - acc: 0.978 - 0s 195ms/step - loss: 0.0384 - acc: 0.9789 - val_loss: 2.6534 - val_acc: 0.4921\n",
      "Epoch 127/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0215 - acc: 1.000 - ETA: 0s - loss: 0.0171 - acc: 1.000 - 0s 193ms/step - loss: 0.0171 - acc: 1.0000 - val_loss: 2.3835 - val_acc: 0.6825\n",
      "Epoch 128/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0172 - acc: 1.000 - ETA: 0s - loss: 0.0180 - acc: 0.994 - 0s 185ms/step - loss: 0.0180 - acc: 0.9947 - val_loss: 2.4390 - val_acc: 0.6825\n",
      "Epoch 129/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0289 - acc: 0.992 - ETA: 0s - loss: 0.0240 - acc: 0.994 - 0s 205ms/step - loss: 0.0240 - acc: 0.9947 - val_loss: 2.4652 - val_acc: 0.6825\n",
      "Epoch 130/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0291 - acc: 0.992 - ETA: 0s - loss: 0.0275 - acc: 0.994 - 0s 176ms/step - loss: 0.0275 - acc: 0.9947 - val_loss: 2.5142 - val_acc: 0.6825\n",
      "Epoch 131/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0233 - acc: 0.992 - ETA: 0s - loss: 0.0170 - acc: 0.994 - 0s 173ms/step - loss: 0.0170 - acc: 0.9947 - val_loss: 2.6375 - val_acc: 0.6667\n",
      "Epoch 132/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0318 - acc: 0.992 - ETA: 0s - loss: 0.0253 - acc: 0.994 - 0s 176ms/step - loss: 0.0253 - acc: 0.9947 - val_loss: 2.8187 - val_acc: 0.6349\n",
      "Epoch 133/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0091 - acc: 1.000 - ETA: 0s - loss: 0.0073 - acc: 1.000 - 0s 178ms/step - loss: 0.0073 - acc: 1.0000 - val_loss: 2.9012 - val_acc: 0.6349\n",
      "Epoch 134/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0133 - acc: 1.000 - ETA: 0s - loss: 0.0267 - acc: 0.989 - 0s 170ms/step - loss: 0.0267 - acc: 0.9895 - val_loss: 3.1170 - val_acc: 0.6190\n",
      "Epoch 135/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0651 - acc: 0.976 - ETA: 0s - loss: 0.0536 - acc: 0.978 - 0s 171ms/step - loss: 0.0536 - acc: 0.9789 - val_loss: 2.7235 - val_acc: 0.6984\n",
      "Epoch 136/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0171 - acc: 0.992 - ETA: 0s - loss: 0.0316 - acc: 0.984 - 0s 188ms/step - loss: 0.0316 - acc: 0.9842 - val_loss: 3.0771 - val_acc: 0.6508\n",
      "Epoch 137/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0167 - acc: 0.992 - ETA: 0s - loss: 0.0130 - acc: 0.994 - 0s 166ms/step - loss: 0.0130 - acc: 0.9947 - val_loss: 3.3614 - val_acc: 0.4762\n",
      "Epoch 138/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0271 - acc: 0.992 - ETA: 0s - loss: 0.0234 - acc: 0.994 - 0s 173ms/step - loss: 0.0234 - acc: 0.9947 - val_loss: 3.2234 - val_acc: 0.4762\n",
      "Epoch 139/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0296 - acc: 0.992 - ETA: 0s - loss: 0.0288 - acc: 0.989 - 0s 174ms/step - loss: 0.0288 - acc: 0.9895 - val_loss: 2.6903 - val_acc: 0.5714\n",
      "Epoch 140/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0568 - acc: 0.968 - ETA: 0s - loss: 0.0512 - acc: 0.973 - 0s 175ms/step - loss: 0.0512 - acc: 0.9737 - val_loss: 2.0480 - val_acc: 0.6984\n",
      "Epoch 141/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0672 - acc: 0.976 - ETA: 0s - loss: 0.0662 - acc: 0.973 - 0s 178ms/step - loss: 0.0662 - acc: 0.9737 - val_loss: 2.5780 - val_acc: 0.6667\n",
      "Epoch 142/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0396 - acc: 0.984 - ETA: 0s - loss: 0.0370 - acc: 0.984 - 0s 178ms/step - loss: 0.0370 - acc: 0.9842 - val_loss: 3.2691 - val_acc: 0.6032\n",
      "Epoch 143/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.1116 - acc: 0.968 - ETA: 0s - loss: 0.0893 - acc: 0.973 - 0s 196ms/step - loss: 0.0893 - acc: 0.9737 - val_loss: 2.4953 - val_acc: 0.6349\n",
      "Epoch 144/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0124 - acc: 1.000 - ETA: 0s - loss: 0.0163 - acc: 1.000 - 0s 200ms/step - loss: 0.0163 - acc: 1.0000 - val_loss: 1.9323 - val_acc: 0.6825\n",
      "Epoch 145/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0648 - acc: 0.976 - ETA: 0s - loss: 0.0512 - acc: 0.984 - 0s 182ms/step - loss: 0.0512 - acc: 0.9842 - val_loss: 2.4287 - val_acc: 0.5714\n",
      "Epoch 146/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0293 - acc: 0.992 - ETA: 0s - loss: 0.0223 - acc: 0.994 - 0s 193ms/step - loss: 0.0223 - acc: 0.9947 - val_loss: 2.9930 - val_acc: 0.4762\n",
      "Epoch 147/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0313 - acc: 0.984 - ETA: 0s - loss: 0.0405 - acc: 0.978 - 0s 200ms/step - loss: 0.0405 - acc: 0.9789 - val_loss: 2.6329 - val_acc: 0.5714\n",
      "Epoch 148/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0291 - acc: 0.992 - ETA: 0s - loss: 0.0261 - acc: 0.994 - 0s 197ms/step - loss: 0.0261 - acc: 0.9947 - val_loss: 1.9775 - val_acc: 0.6984\n",
      "Epoch 149/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0481 - acc: 0.976 - ETA: 0s - loss: 0.0400 - acc: 0.984 - 0s 180ms/step - loss: 0.0400 - acc: 0.9842 - val_loss: 2.3281 - val_acc: 0.6667\n",
      "Epoch 150/150\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0302 - acc: 0.992 - ETA: 0s - loss: 0.0257 - acc: 0.994 - 0s 174ms/step - loss: 0.0257 - acc: 0.9947 - val_loss: 2.8835 - val_acc: 0.6667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d545eeb358>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,\n",
    "         y_train,\n",
    "         batch_size=128,\n",
    "         epochs=150,\n",
    "         validation_data=(x_valid, y_valid),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test accuracy: 0.6666666865348816\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test set\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "# Print test accuracy\n",
    "print('\\n', 'Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
